{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"We intend to produce a model based on BERT to infer capacity of predicting if a tweet can be categorized as an Alert disaster tweet or not.","metadata":{}},{"cell_type":"markdown","source":"Some kernels have already implemented such models :\n\n* egortrushin/nlp-with-disaster-tweets-roberta-using-pytorch         NLP with Disaster Tweets: RoBERTa using PyTorch     Egor Trushin\n\n* ashishsingh226/disaster-tweet-classification-using-distil-bert     Disaster Tweet Classification using Distil Bert     AshishSingh226\n","metadata":{}},{"cell_type":"markdown","source":"In a perspective to contribute to the community, we propose another implementation for BERT with fast-bert api on PyTorch.\n\nThe objective is to try achieve a high accuracy scoring by :\n\n* implementing most appropriate pretrained model\n* fine tuning hyperparameters\n* fine tune optimizer  ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install fast-bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None\n\nimport torch\nfrom fast_bert.data_cls import BertDataBunch\nfrom fast_bert.learner_cls import BertLearner\nfrom fast_bert.data_lm import BertLMDataBunch\nfrom fast_bert.learner_lm import BertLMLearner\n\nfrom fast_bert.metrics import fbeta, roc_auc, accuracy\nfrom fast_bert.prediction import BertClassificationPredictor\nfrom pathlib import Path\nimport pandas as pd\nimport logging\nimport os\nimport pickle\nfrom time import time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport emoji\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nlogger = logging.getLogger()\ndevice_cuda = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.51003Z","iopub.execute_input":"2021-06-23T21:18:42.510328Z","iopub.status.idle":"2021-06-23T21:18:42.518944Z","shell.execute_reply.started":"2021-06-23T21:18:42.510303Z","shell.execute_reply":"2021-06-23T21:18:42.517693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets loading","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv',sep=\",\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.520682Z","iopub.execute_input":"2021-06-23T21:18:42.52095Z","iopub.status.idle":"2021-06-23T21:18:42.566261Z","shell.execute_reply.started":"2021-06-23T21:18:42.520925Z","shell.execute_reply":"2021-06-23T21:18:42.564876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/nlp-getting-started/test.csv',sep=\",\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.567389Z","iopub.execute_input":"2021-06-23T21:18:42.567649Z","iopub.status.idle":"2021-06-23T21:18:42.584936Z","shell.execute_reply.started":"2021-06-23T21:18:42.567619Z","shell.execute_reply":"2021-06-23T21:18:42.583427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total rows in train data: \",train_data.shape[0])\nprint(\"Total columns in train data: \",train_data.shape[1])\nprint(\"-\"*30)\nprint(\"Total rows in test data: \",test_data.shape[0])\nprint(\"Total columns in test data: \",test_data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.586834Z","iopub.execute_input":"2021-06-23T21:18:42.587152Z","iopub.status.idle":"2021-06-23T21:18:42.593689Z","shell.execute_reply.started":"2021-06-23T21:18:42.587127Z","shell.execute_reply":"2021-06-23T21:18:42.592748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.594702Z","iopub.execute_input":"2021-06-23T21:18:42.59492Z","iopub.status.idle":"2021-06-23T21:18:42.626867Z","shell.execute_reply.started":"2021-06-23T21:18:42.594897Z","shell.execute_reply":"2021-06-23T21:18:42.625193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.630131Z","iopub.execute_input":"2021-06-23T21:18:42.630501Z","iopub.status.idle":"2021-06-23T21:18:42.644378Z","shell.execute_reply.started":"2021-06-23T21:18:42.630467Z","shell.execute_reply":"2021-06-23T21:18:42.643637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.646078Z","iopub.execute_input":"2021-06-23T21:18:42.646697Z","iopub.status.idle":"2021-06-23T21:18:42.661151Z","shell.execute_reply.started":"2021-06-23T21:18:42.64665Z","shell.execute_reply":"2021-06-23T21:18:42.660149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration & Cleaning","metadata":{}},{"cell_type":"code","source":"print(train_data.isnull().sum())\nprint(\"-\"*30)\nprint(test_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.662254Z","iopub.execute_input":"2021-06-23T21:18:42.662674Z","iopub.status.idle":"2021-06-23T21:18:42.685078Z","shell.execute_reply.started":"2021-06-23T21:18:42.662634Z","shell.execute_reply":"2021-06-23T21:18:42.683638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We drop keyword and location columns as too many Nan and not really useful in the perspective of this classification objective.","metadata":{}},{"cell_type":"code","source":"train_data.groupby(train_data.target).count().text","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.686494Z","iopub.execute_input":"2021-06-23T21:18:42.686773Z","iopub.status.idle":"2021-06-23T21:18:42.70889Z","shell.execute_reply.started":"2021-06-23T21:18:42.686745Z","shell.execute_reply":"2021-06-23T21:18:42.707804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train_data.target,data = train_data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.710043Z","iopub.execute_input":"2021-06-23T21:18:42.710406Z","iopub.status.idle":"2021-06-23T21:18:42.812263Z","shell.execute_reply.started":"2021-06-23T21:18:42.710373Z","shell.execute_reply":"2021-06-23T21:18:42.811753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is quite balanced between the 2 classes which is ok for our classification purpose.","metadata":{}},{"cell_type":"markdown","source":"### Dropping keyword and location columns","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop(['keyword','location'], axis = 1)\ntest_data = test_data.drop(['keyword','location'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.81315Z","iopub.execute_input":"2021-06-23T21:18:42.813477Z","iopub.status.idle":"2021-06-23T21:18:42.818413Z","shell.execute_reply.started":"2021-06-23T21:18:42.813444Z","shell.execute_reply":"2021-06-23T21:18:42.817815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.819469Z","iopub.execute_input":"2021-06-23T21:18:42.819711Z","iopub.status.idle":"2021-06-23T21:18:42.843272Z","shell.execute_reply.started":"2021-06-23T21:18:42.819688Z","shell.execute_reply":"2021-06-23T21:18:42.8418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"We need to remove superfluous data like url, emoji","metadata":{}},{"cell_type":"code","source":"def cleanTweet(txt):\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    txt = re.sub(r'\\n','',txt)\n    # to remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+','',txt)\n    txt = re.sub(r\"https?://\\S+|www\\.\\S+\",\"\",txt)\n    txt = re.sub(r\"<.*?>\",\"\",txt)\n    return txt  \n\ndef make_Lower(text):\n    return str.lower(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.844921Z","iopub.execute_input":"2021-06-23T21:18:42.845229Z","iopub.status.idle":"2021-06-23T21:18:42.859334Z","shell.execute_reply.started":"2021-06-23T21:18:42.845199Z","shell.execute_reply":"2021-06-23T21:18:42.857694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.text = train_data.text.apply(cleanTweet)\ntrain_data.text = train_data.text.apply(make_Lower)\n\ntest_data.text = test_data.text.apply(cleanTweet)\ntest_data.text = test_data.text.apply(make_Lower)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:42.860893Z","iopub.execute_input":"2021-06-23T21:18:42.861125Z","iopub.status.idle":"2021-06-23T21:18:47.315036Z","shell.execute_reply.started":"2021-06-23T21:18:42.861103Z","shell.execute_reply":"2021-06-23T21:18:47.313931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We save these preprocessed datasets as they will be model input from disk resources","metadata":{}},{"cell_type":"code","source":"train_data.to_csv('train_preprocessed.csv', index=False) \ntest_data.to_csv('test_preprocessed.csv', index=False) ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.316158Z","iopub.execute_input":"2021-06-23T21:18:47.316377Z","iopub.status.idle":"2021-06-23T21:18:47.351273Z","shell.execute_reply.started":"2021-06-23T21:18:47.316352Z","shell.execute_reply":"2021-06-23T21:18:47.350569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"### Define main paths","metadata":{}},{"cell_type":"code","source":"DATA_PATH = Path('data/')\nLABEL_PATH = Path('data/')\nMODEL_PATH=Path('models/')\nLOG_PATH=Path('logs/')\nMODEL_PATH.mkdir(exist_ok=True)\nDATA_PATH.mkdir(exist_ok=True)\nLOG_PATH.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.352157Z","iopub.execute_input":"2021-06-23T21:18:47.352364Z","iopub.status.idle":"2021-06-23T21:18:47.357843Z","shell.execute_reply.started":"2021-06-23T21:18:47.352341Z","shell.execute_reply":"2021-06-23T21:18:47.356544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare train and validation sets","metadata":{}},{"cell_type":"code","source":"val_set = train_data.sample(frac=0.2, replace=False, random_state=42)\ntrain_set = train_data.drop(index = val_set.index)\nprint('Number of tweets in val_set:',len(val_set))\nprint('Number of tweets in train_set:', len(train_set))\nval_set.to_csv(str(DATA_PATH) + '/val_set.csv')\ntrain_set.to_csv(str(DATA_PATH) + '/train_set.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:19:58.568785Z","iopub.execute_input":"2021-06-23T21:19:58.56929Z","iopub.status.idle":"2021-06-23T21:19:58.623435Z","shell.execute_reply.started":"2021-06-23T21:19:58.569258Z","shell.execute_reply":"2021-06-23T21:19:58.622334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_data.columns[2:].to_list()\nwith open(str(LABEL_PATH) + '/labels.txt', 'w') as f:\n    for i in labels:\n        f.write(i + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:20:28.963222Z","iopub.execute_input":"2021-06-23T21:20:28.963774Z","iopub.status.idle":"2021-06-23T21:20:28.970968Z","shell.execute_reply.started":"2021-06-23T21:20:28.963733Z","shell.execute_reply":"2021-06-23T21:20:28.970063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_texts = train_data['text'].to_list()\nprint('Number of tweets:', len(all_texts))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:20:38.457629Z","iopub.execute_input":"2021-06-23T21:20:38.457994Z","iopub.status.idle":"2021-06-23T21:20:38.4649Z","shell.execute_reply.started":"2021-06-23T21:20:38.457959Z","shell.execute_reply":"2021-06-23T21:20:38.463464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create LMDataBunch","metadata":{}},{"cell_type":"markdown","source":"In this phase we first fine tune BERT model to our corpus.\n\nWe first need to instanciate data bunches for batching learning phase.\n\nNote here we use 'bert-base-uncased' tokenizer defined on pretrained BERT model. ","metadata":{}},{"cell_type":"code","source":"t0 = time()\ndatabunch_lm = BertLMDataBunch.from_raw_corpus(\n                    data_dir=Path(DATA_PATH),\n                    text_list=all_texts,\n                    tokenizer='bert-base-uncased',\n                    batch_size_per_gpu=16,\n                    max_seq_length=512,\n                    multi_gpu=False,\n                    model_type='bert',\n                    logger=logger)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:27:20.305208Z","iopub.execute_input":"2021-06-23T21:27:20.305634Z","iopub.status.idle":"2021-06-23T21:27:21.038612Z","shell.execute_reply.started":"2021-06-23T21:27:20.305609Z","shell.execute_reply":"2021-06-23T21:27:21.037976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create LMLearner","metadata":{}},{"cell_type":"markdown","source":"We instanciate a learner for fine tuning the model based on pretrained bert model.\n\nHere we choose simple 'bert' model, and accuracy as metric of learning process.","metadata":{}},{"cell_type":"code","source":"t0 = time()\nmetrics = []\nmetrics.append({'name': 'accuracy', 'function': accuracy})\nlm_learner = BertLMLearner.from_pretrained_model(\n                            dataBunch=databunch_lm,\n                            pretrained_path='bert-base-uncased',\n                            output_dir=Path(MODEL_PATH),\n                            metrics=metrics,\n                            device=device_cuda,\n                            logger=logger,\n                            multi_gpu=False,\n                            logging_steps=50,\n                            fp16_opt_level=\"O2\")\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:28:18.764675Z","iopub.execute_input":"2021-06-23T21:28:18.764962Z","iopub.status.idle":"2021-06-23T21:29:04.948724Z","shell.execute_reply.started":"2021-06-23T21:28:18.764937Z","shell.execute_reply":"2021-06-23T21:29:04.947361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the learner","metadata":{}},{"cell_type":"markdown","source":"We choose AdamW with warmup_cosine that variates learning rate from its original value to 0 with cosine periodicity.","metadata":{}},{"cell_type":"code","source":"\nlm_learner.fit(epochs=5,\n            lr=1e-4,\n            validate=True,\n            schedule_type=\"warmup_cosine\",\n            optimizer_type=\"adamw\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.739165Z","iopub.status.idle":"2021-06-23T21:18:47.739574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nt0 = time()\nlm_learner.validate()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.740533Z","iopub.status.idle":"2021-06-23T21:18:47.740972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\nlm_learner.save_model()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.741837Z","iopub.status.idle":"2021-06-23T21:18:47.742258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create databunch for our classification","metadata":{}},{"cell_type":"code","source":"labels_col = ['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.743185Z","iopub.status.idle":"2021-06-23T21:18:47.7436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\ndatabunch = BertDataBunch(Path(DATA_PATH), Path(LABEL_PATH),\n                          tokenizer='bert-base-uncased',\n                          train_file='train_set.csv',\n                          val_file='val_set.csv',\n                          label_file='labels.txt',\n                          text_col='text',\n                          label_col=labels_col,\n                          batch_size_per_gpu=16,\n                          max_seq_length=512,\n                          multi_gpu=False,\n                          multi_label=True,\n                          model_type='bert')\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.744472Z","iopub.status.idle":"2021-06-23T21:18:47.74492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Création de Learner pour la classification","metadata":{}},{"cell_type":"markdown","source":"Prepare metrics output with accuracy","metadata":{}},{"cell_type":"code","source":"metrics = []\nmetrics.append({'name': 'accuracy', 'function': accuracy})\n","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:38:03.026717Z","iopub.execute_input":"2021-06-24T18:38:03.027288Z","iopub.status.idle":"2021-06-24T18:38:03.111348Z","shell.execute_reply.started":"2021-06-24T18:38:03.027174Z","shell.execute_reply":"2021-06-24T18:38:03.110117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create tensorboard out dir as it is not created straightly","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = Path(str(MODEL_PATH) + '/finetuned_model')\nWGTS_PATH = Path(str(MODEL_PATH) + '/model_out/pytorch_model.bin')\nFINETUNED_MODEL_PATH=Path('models/finetuned_model')\nFINETUNED_MODEL_PATH.mkdir(exist_ok=True)\nFINETUNED_MODEL_TENSORBOARD_PATH = Path(str(MODEL_PATH) + '/finetuned_model/tensorboard')\nOUTPUT_DIR.mkdir(exist_ok=True)\nFINETUNED_MODEL_TENSORBOARD_PATH.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T18:45:42.794481Z","iopub.execute_input":"2021-06-24T18:45:42.794813Z","iopub.status.idle":"2021-06-24T18:45:42.800258Z","shell.execute_reply.started":"2021-06-24T18:45:42.794786Z","shell.execute_reply":"2021-06-24T18:45:42.799245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\ncl_learner = BertLearner.from_pretrained_model(\n                        databunch,\n                        pretrained_path=str(MODEL_PATH) + '/model_out',\n                        metrics=metrics,\n                        device=device_cuda,\n                        logger=logger,\n                        output_dir=Path(OUTPUT_DIR),\n                        finetuned_wgts_path=Path(WGTS_PATH),\n                        warmup_steps=50,\n                        multi_gpu=False,\n                        multi_label=True,\n                        is_fp16=True,\n                        logging_steps=50)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.747509Z","iopub.status.idle":"2021-06-23T21:18:47.747977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Find the optimal learning rate","metadata":{}},{"cell_type":"markdown","source":"Based on integrated Bert Adam optimizer we try to fit the most suitable learning rate for our fine-tuning. \n\nThis step is important as it is fine tuning, we don't want to retrain the whole model but be as accurate as possible.","metadata":{}},{"cell_type":"code","source":"#cl_learner.lr_find(start_lr=1e-5, optimizer_type='adamw',cache_dir=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cl_learner.plot()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\ncl_learner.fit(epochs=5,\n            lr=9e-5,\n            validate=True,\n            schedule_type=\"warmup_cosine\",\n            optimizer_type=\"adamw\")\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.748842Z","iopub.status.idle":"2021-06-23T21:18:47.749252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\ncl_learner.validate()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.750095Z","iopub.status.idle":"2021-06-23T21:18:47.750523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = time()\ncl_learner.save_model()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.751549Z","iopub.status.idle":"2021-06-23T21:18:47.751994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.752957Z","iopub.status.idle":"2021-06-23T21:18:47.753395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prédictions","metadata":{}},{"cell_type":"code","source":"predictor = BertClassificationPredictor(\n                model_path=str(MODEL_PATH) + '/finetuned_model/model_out',\n                label_path=str(LABEL_PATH),\n                multi_label=True,\n                model_type='bert',\n                do_lower_case=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.754187Z","iopub.status.idle":"2021-06-23T21:18:47.754649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### unit test","metadata":{}},{"cell_type":"code","source":"t0 = time()\ntweet = test_data['text'][0]\nt = predictor.predict(tweet)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')\nfor pred in t:\n    print(pred[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.755702Z","iopub.status.idle":"2021-06-23T21:18:47.756126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### general prediction","metadata":{}},{"cell_type":"code","source":"predictions = []\nfor idx, row in test_data.iterrows():\n    t = predictor.predict(row.text)\n    for pred in t:\n        print(pred[0])\n        predictions.append([row.id, pred[0]])\n\ndf = pd.DataFrame(predictions, columns=['id','target'])        \ndf.to_csv('Submission.csv', index=False)        ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:18:47.75716Z","iopub.status.idle":"2021-06-23T21:18:47.757571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}